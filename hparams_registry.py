#hprams_registry.py


#Registers a hyperparameter in the hparams dictionary.
#Each hyperparameter has:
#hparam_name → Name of the hyperparameter.
#default_val → Default value.
#random_val_fn → Function to generate a random value.


import numpy as np
from model_training import misc


def _define_hparam(hparams, hparam_name, default_val, random_val_fn):
    hparams[hparam_name] = (hparams, hparam_name, default_val, random_val_fn)

def _hparams(algorithm, dataset, random_seed): #Defines a hyperparameter depending on the dataset and algorithm
    """
    Global registry of hyperparams. Each entry is a (default, random) tuple.
    New algorithms / networks / etc. should add entries here.
    """
    IMAGE_DATASETS = ['MIMIC', 'CheXpert']

    hparams = {}

    def _hparam(name, default_val, random_val_fn): #To define a specific hyperparameter
        """Define a hyperparameter. random_val_fn takes a RandomState and
        returns a random hyperparameter value."""
        assert name not in hparams #Avoids duplicates
        random_state = np.random.RandomState( #Generates a random value
            misc.seed_hash(random_seed, name)
        )
        hparams[name] = (default_val, random_val_fn(random_state))

    
     #=====================Common hyperparameters for all algorithm=====================
    
    #_hparam(name, default_val, random_val_fn)
    _hparam('nonlinear_classifier', False, lambda r: bool(r.choice([False, False]))) #default->usea linear classfier
    _hparam('pretrained', True, lambda r: True) #net pretarined with ImageNet weights
    _hparam('last_layer_dropout', 0., lambda r: 0.)
    _hparam('lr', 1e-3, lambda r: 10**r.uniform(-4, -2)) #learning rate
    _hparam('weight_decay', 1e-4, lambda r: 10**r.uniform(-6, -3))
    #_hparam('batch_size', 64, lambda r: 64) #ERM
    _hparam('batch_size', 32, lambda r: 32) #It was reduced for ISNetDANN

    #=====================Common hyperparameters for all algorithm=======================

    
    #===================Dataset-and-algorithm-specific hparam definitions=================

    #------ERM--------
    if algorithm == 'ERM': 
        _hparam('optimizer', 'adam', lambda r: 'adam')
    #------ERM--------

    
    #-------DANN--------
    if algorithm == 'DANN':
        _hparam('lr_g', 1e-3, lambda r: 10**r.uniform(-4, -2)) #learining rate generator
        _hparam('lr_d', 1e-3, lambda r: 10**r.uniform(-4, -2)) #learining rate discriminator
        _hparam('optimizer', 'adam', lambda r: 'adam') 
        _hparam('lambda', 1.0, lambda r: 10**r.uniform(-2, 2)) #adversarial weighting parameter
        _hparam('weight_decay_d', 0., lambda r: 10**r.uniform(-6, -2)) #L2 regularization for the discriminator.
        _hparam('weight_decay_g', 0., lambda r: 10**r.uniform(-6, -2)) #L2 regularization for the generator (feature extractor).
        _hparam('d_steps_per_g_step', 1, lambda r: int(2**r.uniform(0, 3))) #Number of discriminator training steps per generator step.
        _hparam('grad_penalty', 0., lambda r: 10**r.uniform(-2, 1)) #gradient penalty
        _hparam('mlp_width', 256, lambda r: int(2**r.uniform(7, 10))) #number of neurons of each hidden layer of the MLP
        _hparam('mlp_depth', 3, lambda r: int(r.choice([3, 4, 5]))) #Network depth (number of hidden layers of the MLP)
        _hparam('mlp_dropout', 0., lambda r: r.choice([0., 0.1, 0.5])) #Dropout probability in the MLP
    #-------DANN--------
    
   
    #------ISNet--------
    if algorithm == 'ISNetAlgorithm':
                
        _hparam('optimizer', 'sgd', lambda r: 'sgd')
        _hparam('heat', True, lambda r: True) #activate heatmap analysis

        #Explainability weighting parameter 
        #_hparam('P', 0.7, lambda r: 0.7) #fixed value
        _hparam('P', {0:0.0, 2:0.2, 4:0.4, 6:0.7}, lambda r: {0:0.0, 2:0.2, 4:0.4, 6:0.7}) ##increse for 10 epochs
        #C1 and C2 parameters, should define natural range of total absolute heatmap relevance
        _hparam('cut', 1, lambda r: 1) #default value, min
        _hparam('cut2', 25, lambda r: 25) #default value, max

        #values to initialize the tune cut
        #_hparam('cut', 1e-5, lambda r: 1e-5)
        #_hparam('cut2', 1000.0, lambda r: 1000.0)
        #values to initialize the tune cut
        
        _hparam('A', 1, lambda r: 1)
        _hparam('B', 1, lambda r: 1)
        _hparam('Ea', 1, lambda r: 1)
        _hparam('d', 0.9, lambda r: 0.9)
        _hparam('e', 1e-2, lambda r: 1e-2)
        _hparam('clip', 1.0, lambda r: 1.0) 
        _hparam('Zb', 1, lambda r: 1)
    
        _hparam('selective', True, lambda r: True) #Selective ISNet -> 1 heatmap generated by explaining a Softmax-based quantity (η₍c₎) computed for the class associated with the lowest logit.
        _hparam('highest', 1, lambda r: 1) #single heatmap, for highest logit, not used for training
        _hparam('random', False, lambda r: False)
    
        _hparam('tuneCutEpochs', 3, lambda r: 3) # epochs to calculate c1 and c2, if activated
        _hparam('multiple', False, lambda r: False) #one LRP heatmap per sample
        _hparam('norm', True, lambda r: True) 
        _hparam('detach', True, lambda r: True) 
        _hparam('momentum', 0.9, lambda r: 0.9)
        _hparam('alternativeForeground', 'L2', lambda r: 'L2')
        _hparam('selectiveEps', 0.01, lambda r: 0.01)
        
        _hparam('penalizeAll', False, lambda r: False)
        _hparam('dLoss', 0.8, lambda r: 0.8)
        _hparam('dropout', False, lambda r: False)
         
    #------ISNet--------


    #------ISNetDANN--------
    if algorithm == 'ISNetDANN':
        
        _hparam('optimizer', 'sgd', lambda r: 'sgd')

        #--ISNet--
        
        #activate heatmap analysis
        _hparam('heat', True, lambda r: True)

        #Explainability weighting parameter
        #_hparam('P', 0.7, lambda r: 0.7) #fixed value
        #_hparam('P', {0:0.0, 2:0.2, 4:0.4, 6:0.7}, lambda r: {0:0.0, 2:0.2, 4:0.4, 6:0.7}) #increse for 10 epochs
        _hparam('P', None, lambda r: None) #increses during training 0.05 -> 0.4

        #C1 and C2 parameters, should define natural range of total absolute heatmap relevance
        
        #----Suggested values for DenseNet------
        _hparam('cut', 1, lambda r: 1)
        _hparam('cut2', 25, lambda r: 25)
        #----Suggested values for DenseNet------

        #----cut values computed with 5 epochs---
        #_hparam('cut', 7.729, lambda r: 7.729) 
        #_hparam('cut2', 93.008, lambda r: 93.008)
        #----cut values computed with 5 epochs---
        
        _hparam('A', 1, lambda r: 1)
        _hparam('B', 1, lambda r: 1)
        _hparam('Ea', 1, lambda r: 1)
        _hparam('d', 0.9, lambda r: 0.9)
        _hparam('e', 1e-2, lambda r: 1e-2)
        _hparam('clip', 1.0, lambda r: 1.0)
        _hparam('Zb', 1, lambda r: 1)
    
        _hparam('selective', True, lambda r: True) # #Selective ISNet -> 1 heatmap generated by explaining a Softmax-based quantity (η₍c₎) computed for the class associated with the lowest logit.
        _hparam('highest', 1, lambda r: 1) #single heatmap, for highest logit, not used for training
        _hparam('random', False, lambda r: False)

        # epochs to calculate c1 and c2, if activated
        #_hparam('tuneCutEpochs', 5, lambda r: 5)     
        _hparam('tuneCutEpochs', None, lambda r: None)
        
        _hparam('multiple', False, lambda r: False) #one LRP heatmap per sample
        _hparam('norm', True, lambda r: True) #used by Compound loss
        _hparam('detach', True, lambda r: True) 
        _hparam('momentum', 0.9, lambda r: 0.9)
        _hparam('alternativeForeground', 'L2', lambda r: 'L2')
        _hparam('selectiveEps', 0.01, lambda r: 0.01)
        
        _hparam('penalizeAll', False, lambda r: False)
        _hparam('dLoss', 0.8, lambda r: 0.8)
        _hparam('dropout', False, lambda r: False) 

        #--DANN--
        
        _hparam('lr_g', 1e-3, lambda r: 10**r.uniform(-4, -2)) #learining rate generator
        _hparam('lr_d', 1e-3, lambda r: 10**r.uniform(-4, -2)) #learining rate discriminator
       
        _hparam('weight_decay_d', 0., lambda r: 10**r.uniform(-6, -2)) #L2 regularization for the discriminator
        _hparam('weight_decay_g', 0., lambda r: 10**r.uniform(-6, -2)) #L2 regularization for the generator (feature extractor)
        _hparam('d_steps_per_g_step', 1, lambda r: int(2**r.uniform(0, 3))) #Number of discriminator training steps per generator step
        _hparam('grad_penalty', 0., lambda r: 10**r.uniform(-2, 1)) #gradient penalty
        
        #for sex
        _hparam('lambda', 0.2, lambda r: 10**r.uniform(-2, 2))
        _hparam('mlp_width', 256, lambda r: int(2**r.uniform(7, 10))) #number of neurons of each hidden layer of the MLP
        _hparam('mlp_depth', 3, lambda r: int(r.choice([3, 4, 5]))) #Network depth (number of hidden layers of the MLP)
        _hparam('mlp_dropout', 0., lambda r: r.choice([0., 0.1, 0.5])) #Dropout probability in the MLP
        #for sex
    
        #for race
        #_hparam('lambda', 0.4, lambda r: 10**r.uniform(-2, 2))
        #_hparam('mlp_width', 512, lambda r: int(2**r.uniform(7, 10))) #number of neurons of each hidden layer of the MLP
        #_hparam('mlp_depth', 4, lambda r: int(r.choice([3, 4, 5])))#Network depth (number of hidden layers of the MLP)
        #_hparam('mlp_dropout', 0.2, lambda r: r.choice([0., 0.1, 0.5])) #Dropout probability in the MLP
        #for race
    
    #------ISNetDANN--------
    

    return hparams

    #===================Dataset-and-algorithm-specific hparam definitions=================

def default_hparams(algorithm, dataset): #Returns the default values
    return {a: b for a, (b, c) in _hparams(algorithm, dataset, 0).items()}


def random_hparams(algorithm, dataset, seed): #Returns the generated random values
    return {a: c for a, (b, c) in _hparams(algorithm, dataset, seed).items()}

